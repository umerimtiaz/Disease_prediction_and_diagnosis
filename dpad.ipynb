{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as pltly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 16, 'name': 'Breast Cancer Wisconsin (Prognostic)', 'repository_url': 'https://archive.ics.uci.edu/dataset/16/breast+cancer+wisconsin+prognostic', 'data_url': 'https://archive.ics.uci.edu/static/public/16/data.csv', 'abstract': 'Prognostic Wisconsin Breast Cancer Database', 'area': 'Health and Medicine', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 198, 'num_features': 33, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Outcome'], 'index_col': ['ID'], 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1995, 'last_updated': 'Sun Jan 14 2024', 'dataset_doi': '10.24432/C5GK50', 'creators': ['William Wolberg', 'W. Street', 'Olvi Mangasarian'], 'intro_paper': None, 'additional_info': {'summary': 'Each record represents follow-up data for one breast cancer case.  These are consecutive patients seen by Dr. Wolberg since 1984, and include only those cases exhibiting invasive breast cancer and no evidence of distant metastases at the time of diagnosis. \\r\\n\\r\\nThe first 30 features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/\\r\\n\\r\\nThe separation described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\\r\\n\\r\\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: \\r\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\\r\\n\\r\\nThe Recurrence Surface Approximation (RSA) method is a linear programming model which predicts Time To Recur using both recurrent and nonrecurrent cases.  See references (i) and (ii) above for details of the RSA method. \\r\\n\\r\\nThis database is also available through the UW CS ftp server:\\r\\n\\r\\nftp ftp.cs.wisc.edu\\r\\ncd math-prog/cpo-dataset/machine-learn/WPBC/', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1) ID number\\n2) Outcome (R = recur, N = nonrecur)\\n3) Time (recurrence time if field 2 = R, disease-free time if \\n\\tfield 2\\t= N)\\n4-33) Ten real-valued features are computed for each cell nucleus:\\n\\n\\ta) radius (mean of distances from center to points on the perimeter)\\n\\tb) texture (standard deviation of gray-scale values)\\n\\tc) perimeter\\n\\td) area\\n\\te) smoothness (local variation in radius lengths)\\n\\tf) compactness (perimeter^2 / area - 1.0)\\n\\tg) concavity (severity of concave portions of the contour)\\n\\th) concave points (number of concave portions of the contour)\\n\\ti) symmetry \\n\\tj) fractal dimension (\"coastline approximation\" - 1)\\n\\n34) Tumor size - diameter of the excised tumor in centimeters\\n35) Lymph node status - number of positive axillary lymph nodes\\nobserved at time of surgery', 'citation': None}}\n",
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID      Integer        None        None  None   \n",
      "1                 Time  Feature      Integer        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature      Integer        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "32          tumor_size  Feature   Continuous        None        None  None   \n",
      "33   lymph_node_status  Feature      Integer        None        None  None   \n",
      "34             Outcome   Target  Categorical        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n",
      "32             no  \n",
      "33            yes  \n",
      "34             no  \n"
     ]
    }
   ],
   "source": [
    "# Importing data from the source\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_prognostic = fetch_ucirepo(id=16) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_prognostic.data.features \n",
    "y = breast_cancer_wisconsin_prognostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "source_metadata = breast_cancer_wisconsin_prognostic.metadata\n",
    "print(source_metadata) \n",
    "  \n",
    "# variable information \n",
    "list_variables = breast_cancer_wisconsin_prognostic.variables\n",
    "print(list_variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (198, 33)\n",
      "lymph_node_status has null value count  4\n",
      "list of null value columns ['lymph_node_status']\n",
      "lymph_node_status has null value count  0 after treating missing values as mean of the data\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape\", X.shape)\n",
    "\n",
    "# Handling Null/NaN missing values\n",
    "null_value_variables = np.array([]) # initialize an empty array to store the variables with null value\n",
    "for col in X:\n",
    "    if(X[col].isnull().sum() != 0):\n",
    "        print(col,\"has null value count \", X[col].isnull().sum())\n",
    "        X = X.fillna(int(X[col].mean())) # fill missing value by taking int(mean) of that column\n",
    "        \n",
    "        null_value_variables = np.append(null_value_variables, col)   \n",
    "\n",
    "print(\"list of null value columns\",null_value_variables)\n",
    "\n",
    "# After null / nan value treatment\n",
    "for null_value_variable in null_value_variables:\n",
    "    print(null_value_variable,\"has null value count \",X[null_value_variable].isnull().sum(), \"after treating missing values as mean of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transforming categorical data N and R to integers 0s and 1s\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit label encoder and transform target variable\n",
    "#print(y)\n",
    "y_dash = np.ravel(y)\n",
    "#print(y_dash)\n",
    "#Label Encoding the features (N as 0,R as 1)\n",
    "y_encoded = label_encoder.fit_transform(y_dash)\n",
    "#print(y_encoded)\n",
    "# splitting the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.20, random_state=42)\n",
    "\n",
    "#Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier()]\n",
    "model_names =['Logistic_Regression', 'SVM', 'Decision_Tree_Classifier', 'Random_Forest_Classifier', 'KNN']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LogisticRegression() {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      " SVC() {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\n",
      " DecisionTreeClassifier() {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}\n",
      "\n",
      " RandomForestClassifier() {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      " KNeighborsClassifier() {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tunning\n",
    "# Understanding all of the hyperparameters for parameter selection for GridSearchCV()\n",
    "\n",
    "# listing all the parameters for our selected models\n",
    "for model, model_name in zip(models, model_names):\n",
    "    print(\"\\n\", model, model.get_params())\n",
    "\n",
    "#Setting up dictionaries for the parameters for models \n",
    "param_grid_lrc = {'fit_intercept': [True, False]}\n",
    "param_grid_svc = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "param_grid_dtc = {'max_depth': list(range(1, 30))}  \n",
    "param_grid_rfc = {}  \n",
    "param_grid_knc = {'n_neighbors': list(range(1,15))}  \n",
    "\n",
    "# list of dictionaries\n",
    "param_grid_list = [param_grid_lrc, param_grid_svc, param_grid_dtc, param_grid_rfc, param_grid_knc]\n",
    "# print(param_grid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model:  Logistic_Regression.joblib\n",
      "Saving Model:  SVM.joblib\n",
      "Saving Model:  Decision_Tree_Classifier.joblib\n",
      "Saving Model:  Random_Forest_Classifier.joblib\n",
      "Saving Model:  KNN.joblib\n"
     ]
    }
   ],
   "source": [
    "# Generating and saving models\n",
    "import joblib\n",
    "\n",
    "for model, model_name, param_grid in zip(models, model_names, param_grid_list):\n",
    "    # fit the model using GridSearchCV Grid Search Cross Validation\n",
    "    grid_search_model = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='f1')\n",
    "\n",
    "    # training the model on the dataset\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "\n",
    "    # saving the model using joblib\n",
    "    model_file_name = model_name+\".joblib\"\n",
    "    print(\"Saving Model: \", model_file_name)\n",
    "    joblib.dump(grid_search_model, model_file_name)\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model:  Logistic_Regression.joblib\n",
      "Running test score >>>> working\n",
      "Working Successfull >>>> Logistic_Regression \n",
      "\n",
      "Loading Model:  SVM.joblib\n",
      "Running test score >>>> working\n",
      "Working Successfull >>>> SVM \n",
      "\n",
      "Loading Model:  Decision_Tree_Classifier.joblib\n",
      "Running test score >>>> working\n",
      "Working Successfull >>>> Decision_Tree_Classifier \n",
      "\n",
      "Loading Model:  Random_Forest_Classifier.joblib\n",
      "Running test score >>>> working\n",
      "Working Successfull >>>> Random_Forest_Classifier \n",
      "\n",
      "Loading Model:  KNN.joblib\n",
      "Running test score >>>> working\n",
      "Working Successfull >>>> KNN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading model and running score analysis\n",
    "models_scores_accuracy = []\n",
    "models_scores_f1_score = []\n",
    "models_scores_precision_score = []\n",
    "models_scores_recall_score = []\n",
    "models_scores_confusion_matrix = []\n",
    "\n",
    "for model, model_name, param_grid in zip(models, model_names, param_grid_list):\n",
    "        \n",
    "    # loading the model and run for scores - score calculation\n",
    "    model_file_name = model_name+\".joblib\"\n",
    "    print(\"Loading Model: \", model_file_name)\n",
    "    grid_search_model = joblib.load(model_file_name)\n",
    "\n",
    "    # Making predictions using test data\n",
    "    y_pred = grid_search_model.predict(X_test)\n",
    "    \n",
    "    print(\"Running test score >>>> working\")\n",
    "    # Calculate accuracy score for all models and storing in an array\n",
    "    model_accuracy = accuracy_score(y_test, y_pred)\n",
    "    models_scores_accuracy.append([model_name, model_accuracy])\n",
    "\n",
    "    # Calculate f1 score for all models and storing in an array\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    models_scores_f1_score.append([model_name, model_f1_score])\n",
    "\n",
    "    # Calculate precision score for all models and storing in an array\n",
    "    model_precision_score = precision_score(y_test, y_pred)\n",
    "    models_scores_precision_score.append([model_name, model_precision_score])\n",
    "\n",
    "    # Calculate recall score for all models and storing in an array\n",
    "    model_recall_score = recall_score(y_test, y_pred)\n",
    "    models_scores_recall_score.append([model_name, model_recall_score])\n",
    " \n",
    "    # Calculate confusion matrix for all models and storing in an array\n",
    "    model_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    models_scores_confusion_matrix.append([model_name, model_confusion_matrix])\n",
    "\n",
    "    print(\"Working Successfull >>>>\", model_name, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sorted Model #accuracy scores:\n",
      " [['Random_Forest_Classifier', 0.875], ['SVM', 0.8], ['Decision_Tree_Classifier', 0.8], ['KNN', 0.775], ['Logistic_Regression', 0.625]]\n",
      "\n",
      "sorted Model #f1 scores:\n",
      " [['Random_Forest_Classifier', 0.5454545454545454], ['SVM', 0.5], ['Decision_Tree_Classifier', 0.5], ['KNN', 0.47058823529411764], ['Logistic_Regression', 0.44444444444444436]]\n",
      "\n",
      "sorted Model #precision scores:\n",
      " [['Random_Forest_Classifier', 1.0], ['SVM', 0.5], ['Decision_Tree_Classifier', 0.5], ['KNN', 0.4444444444444444], ['Logistic_Regression', 0.3157894736842105]]\n",
      "\n",
      "sorted Model #recall scores:\n",
      " [['Logistic_Regression', 0.75], ['SVM', 0.5], ['Decision_Tree_Classifier', 0.5], ['KNN', 0.5], ['Random_Forest_Classifier', 0.375]]\n",
      "\n",
      " Logistic_Regression \n",
      " [[19 13]\n",
      " [ 2  6]]\n",
      "\n",
      " SVM \n",
      " [[28  4]\n",
      " [ 4  4]]\n",
      "\n",
      " Decision_Tree_Classifier \n",
      " [[28  4]\n",
      " [ 4  4]]\n",
      "\n",
      " Random_Forest_Classifier \n",
      " [[32  0]\n",
      " [ 5  3]]\n",
      "\n",
      " KNN \n",
      " [[27  5]\n",
      " [ 4  4]]\n"
     ]
    }
   ],
   "source": [
    "# Analyzing test scores\n",
    "sorted_models_accuracy = sorted(models_scores_accuracy, key = lambda x: x[1], reverse=True )\n",
    "print(\"\\nsorted Model #accuracy scores:\\n\", sorted_models_accuracy)\n",
    "\n",
    "sorted_models_f1_score = sorted(models_scores_f1_score, key = lambda x: x[1], reverse=True )\n",
    "print(\"\\nsorted Model #f1 scores:\\n\", sorted_models_f1_score)\n",
    "\n",
    "sorted_models_precision_score = sorted(models_scores_precision_score, key = lambda x: x[1], reverse=True )\n",
    "print(\"\\nsorted Model #precision scores:\\n\", sorted_models_precision_score)\n",
    "\n",
    "sorted_models_recall_score = sorted(models_scores_recall_score, key = lambda x: x[1], reverse=True )\n",
    "print(\"\\nsorted Model #recall scores:\\n\", sorted_models_recall_score)\n",
    "\n",
    "for cm in models_scores_confusion_matrix:\n",
    "    print(\"\\n\", cm[0], \"\\n\",cm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
